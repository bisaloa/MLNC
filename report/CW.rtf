{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf210
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww16280\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural

\f0\fs24 \cf0 \
Section A Q1\
GetTrace\
\
Can you explain the output?\
\
The function GetTrace generates a random episode from a given MDP. Each row in the output has three columns, which represents the reward for a transition from the previous state to the current one having taken a particular action, the current state, and the action taken in the current state to move to the following one.\
\
In general, the function GetTrace generates a random sample of the current state taken into account the previous state and the action taken in that state. After that, it gets the reward corresponding to that transition and action. If the current state is not an absorbing one, it generates a random sample of the action in that state. However, if the current state is an absorbing one, none further actions are taken and therefore, the trace ends. The row corresponding to this state is formed by the reward, the state itself, and the action taken in the current state.\
\
For the particular example of the Stair Climbing MDP, the first state is four, because the function StairClimbingMDP assigns a probability of one to this state as the first state. The action in this state is \'93Left\'94, but it could have been \'93Right\'94, as the unbiased policy is used. In the following row, the state is three, since the probability of going from state four to three having taken action \'93Left\'94 is one. The reward for this transition is one and the next action \'93Right\'94, but it could have been \'93Left\'94. The following rows are computed in the same way, until an absorbing state is reached.\
\
In the case of the first row, there is no transition to get to the current state, i.e. it is the first state. Hence, there is no reward for getting to that particular state. That is the reason why a dummy value is assigned to the first reward.\
\
In an absorbing state, the state where the trace ends, no further actions are taken. Therefore, the value for the last action is a dummy one.\
\
Section A Q2\
\
\
\
\
}